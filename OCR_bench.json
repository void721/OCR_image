[
    {
        "image": "9.png",
        "content": "Abstract. In the realms of computer vision and natural language pro- cessing; Large Vision-Language Models (LVLMs) have become indispens- able tools, proficient in generating textual descriptions based on visual inputs. Despite their advancements, our investigation reveals a notewor- thy bias in the generated content, where the output is primarily influ- enced by the underlying Large Language Models (LLMs) prior rather than the input image. Our empirical experiments underscore the persis- tence of this bias, as LVLMs often provide confident answers even in the absence of relevant images or given incongruent visual input. To rectify these biases and redirect the model's focus toward vision information, we introduce two simple, training-free strategies  Firstly, for tasks such as classification or multi-choice question-answering (QA), we propose \"calibration\" step through affine transformation to adjust the output distribution_ This 'Post-Hoc debias\"  approach ensures uniform scores for each answer when the image is absent, serving as an effective regu- larization technique to alleviate the influence of LLM priors. For more intricate open-ended generation tasks, we extend this method to 'De-bias sampling\"' drawing inspirations from contrastive decoding methods: "
    },
    {
        "image": "6.png",
        "content": "An ideal model for dense video captioning predicting captions localized temporally in a video should be able to handle input videos, predict rich, detailed textual descriptions, and be able to produce outputs before pro- cessing the entire video. Current state-of-the-art models, however; process a fixed number of downsampled frames; and make a single full prediction after seeing the whole video. We propose a streaming dense video captioning model that consists of two novel components: First; we propose a new memory module, based on clustering incom- tokens, which can handle arbitrarily videos as the memory is of a fixed size. Second, we develop a streaming decoding algorithm that enables our model to make pre- dictions before the entire video has been processed Our model achieves this streaming ability and significantly im- proves the state-of-the-art on three dense video captioning benchmarks: Activity Net, YouCook2 and ViTT: Our code is released at https:Ilgithubcom/google-researchlscenic. long long ing "
    },
    {
        "image": "2.png",
        "content": "Despite significant development in video transformers, excessive dundant tokens result in high computational costs and reduced ef- ficiency. To address this issue, we propose HaltingVT, an adaptive efficient video transformer architecture with a token halting mech- anism: Unlike methods that enhance the efficiency of video CNN models via frame extraction or early-exit; we optimize the joint video transformer from the perspective of dynamic networks with adaptive token halting mechanism. Additionally, we propose the Glimpser module and Motion achieving a balance between effectiveness and efficiency in an end-to-end training process These approaches can also be extented to other video transformers  Look- ahead, we plan to apply HaltingVT to additional video analysis tasks that demand high efficiency: re-prior Loss, ing "
    },
    {
        "image": "10.png",
        "content": "In this paper; we improve the efficiency of Large Multimodal Models (LMMs) from the perspective of reducing the quantity of visual tokens. By leveraging the redundancy in visual tokens; we proposed a plug-and-play token reduction module that employs the similarity between the class token and tokens as a criterion for pruning and merging visual tokens spatial key "
    },
    {
        "image": "4.png",
        "content": "Given the effectiveness of CLIP Score in detecting hallucination, we propose CLIP-Guided Decoding (CGD) to reduce hallucination by CLIP as vision-language guidance to perfer visually grounded content during generation: The algorithm involves two parts: Reliability Scoring, which designs a scoring function aiming to prioritize candidate responses which are less likely to be hallucinated, and Guided Sentence Generation, which generates responses based on this scoring function. We decode in a similar way to beam search, but at the sentence level: this allows us to apply CLIP scoring on full sentences instead of incomplete words or phrases which would be present when decoding at the token level. using "
    },
    {
        "image": "7.png",
        "content": "In this paper; we tackle the object hallucination issue in LVLMs. We conducted an in-depth analysis of how visual uncertainty influences hallucinations, particularly from the aspect of statistical biases and language priors. Our find- ings indicate that visual uncertainty amplifies these factors, contributing to more hallucinations. In light of this, we intro-duced Visual Contrastive Decoding (VCD), a novel, training- free method that employs contrastive distributions to calibrate the model's output without the usage of external tools. Our extensive experiments across multiple benchmarks and LVLM families confirm VCD's efficacy in reducing hallu- cinations and also demonstrate its potential to enhance the overall perception capabilities of LVLMs. "
    },
    {
        "image": "1.png",
        "content": "This paper proposes image-biased decoding (IBD), a novel decoding method that aims at alleviating the issue of hallucinations in LVLMs. Our approach involves conducting a prediction contrast between the original model and an image biased model to amplify the accurate information associated with image content, thereby improving the factuality of the generated text: In addition, we design a dynamic adjustment strategy that flexibly handles different types of vocabulary: Experimental results demonstrate the high effectiveness of ID in addressing LVLM's hallucination challenges We hope our innovative method and detailed statistical analysis can provide valuable insights for future research in this field: "
    },
    {
        "image": "8.png",
        "content": "We conduct a series of extensive experiments to demonstrate that: (i) the pro posed debiasing methods significantly alleviate hallucination and improve rea soning capability; (ii) exploring the optimal generation configuration unleashes the full potential of existing LVLMs, resulting in substantial performance im- provements compared to default configurations; (iii) the debiasing methods rec- tify model predictions, particularly in situations where the model lacks confidence and is prone to errors; and (iv) we scrutinize the failure cases of our methods, revealing certain flaws in current evaluation benchmarks. "
    },
    {
        "image": "3.png",
        "content": "Despite achieving rapid developments and with widespread applications, Large Vision-Language Models (LVLMs) confront a serious challenge of prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations In this paper; we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text: We further conduct a comprehensive statistical analysis to validate the reliability of our method, and de-sign an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evalu- ation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can signif- icantly reduce hallucinations in LVLMs and en- hance the truthfulness of the generated response being "
    },
    {
        "image": "5.png",
        "content": "We have proposed a streaming model for dense video captioning with two novel components: A clustering-based memory that can efficiently handle arbitrarily long videos with bounded computation, and a streaming decoding algo-rithm that enables our model to make predictions before the entire video has been processed. We achieve this streaming ability while also improving the state-of-the-art o five dense and paragraph-captioning tasks Future work is to develop a benchmark for dense video captioning  which requires reasoning over longer  videos than current  datasets, to better evaluate the abilities of streaming models such as ours. "
    }
]